# WebLLM Configuration for Model Conversion and Deployment

# Model Information
model:
  model_id: "PersonalizedQwen-0.6B-Chat-q4f16_1-MLC"
  base_model: "Qwen/Qwen-0.5B-Chat"
  quantization: "q4f16_1"
  
# Conversation Settings
conversation:
  conv_template: "qwen"
  context_window_size: 2048
  prefill_chunk_size: 512
  sliding_window_size: -1  # Disabled
  
# Performance Optimization
performance:
  max_batch_size: 1
  max_total_sequence_length: 2048
  tensor_parallel_shards: 1
  
# WebGPU Compilation Settings
compilation:
  device: "webgpu"
  target: "webgpu"
  opt_level: "O3"
  required_features:
    - "shader-f16"
  
  # Memory optimization
  memory_planning: true
  memory_pool_size_mb: 1024
  
  # Compilation flags
  compile_flags:
    - "--enable-vulkan"
    - "--enable-webgpu"
  
# Model Library Settings
model_library:
  library_name: "{model_id}-webgpu.wasm"
  compress_library: true
  
# Deployment URLs (Update these with your actual URLs)
deployment:
  model_url: "https://huggingface.co/your-username/{model_id}"
  model_lib_url: "https://github.com/your-username/your-repo/releases/download/v1.0/{model_id}-webgpu.wasm"
  
  # CDN alternatives
  cdn_base_url: "https://cdn.jsdelivr.net/gh/your-username/your-repo@main/models/"
  
# Browser Compatibility
browser:
  minimum_chrome_version: 113
  minimum_firefox_version: 110  # Experimental support
  minimum_safari_version: 16.4
  
  # Feature detection
  required_webgpu_features:
    - "shader-f16"
    - "timestamp-query"
  
  # Memory requirements
  minimum_gpu_memory_mb: 512
  recommended_gpu_memory_mb: 1024

# Model Variants (for different use cases)
variants:
  # Standard variant
  standard:
    quantization: "q4f16_1"
    context_window: 2048
    prefill_chunk_size: 512
    
  # Memory-optimized variant
  memory_optimized:
    quantization: "q4f16_1"
    context_window: 1024
    prefill_chunk_size: 256
    
  # Performance variant
  performance:
    quantization: "q8f16_1"
    context_window: 2048
    prefill_chunk_size: 1024

# Runtime Configuration
runtime:
  # Inference settings
  temperature: 0.7
  top_p: 0.9
  top_k: 40
  repetition_penalty: 1.1
  max_gen_len: 512
  
  # Streaming settings
  streaming: true
  stream_interval: 2
  
  # Error handling
  max_retries: 3
  timeout_ms: 30000
  
# Logging and Debugging
debug:
  enable_logging: false
  log_level: "INFO"
  performance_monitoring: true
  
  # Profiling
  enable_profiling: false
  profile_memory: false
  profile_compute: false

# Security Settings
security:
  # Content filtering
  enable_content_filter: false
  content_filter_threshold: 0.8
  
  # Rate limiting
  enable_rate_limiting: false
  max_requests_per_minute: 60
  
# Application Integration
integration:
  # Web worker support
  use_web_worker: true
  worker_script_url: "webllm-worker.js"
  
  # Offline support
  enable_offline_mode: true
  cache_strategy: "aggressive"
  
  # Service worker
  enable_service_worker: false
  sw_cache_name: "webllm-cache-v1"

# Model Serving Options
serving:
  # Local serving
  local_serving:
    enable: true
    port: 8080
    cors_enabled: true
    
  # Remote serving
  remote_serving:
    enable: false
    endpoint: "https://api.yourservice.com/v1/chat"
    api_key_required: false

# Analytics and Monitoring
analytics:
  # Usage tracking
  enable_usage_tracking: false
  analytics_endpoint: "https://analytics.yourservice.com/events"
  
  # Performance metrics
  track_inference_time: true
  track_memory_usage: true
  track_gpu_utilization: true
  
  # Error reporting
  enable_error_reporting: false
  error_reporting_endpoint: "https://errors.yourservice.com/report"